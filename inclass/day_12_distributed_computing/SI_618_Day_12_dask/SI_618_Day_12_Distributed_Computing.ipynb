{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "This notebook is based on the [Dask tutorial](http://tutorial.dask.org/) and [Dask documentation](http://dask.pydata.org/en/latest/).\n",
    "\n",
    "# Dask DataFrames\n",
    "\n",
    "\n",
    "## When to use `dask.dataframe`\n",
    "\n",
    "Pandas is great for tabular datasets that fit in memory. Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM. The demo dataset we're working with is only about 200MB, so that you can download it in a reasonable time, but `dask.dataframe` will scale to  datasets much larger than memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `dask.dataframe`?\n",
    "\n",
    "The `dask.dataframe` module implements a blocked parallel `DataFrame` object that mimics a large subset of the Pandas `DataFrame` API. One Dask `DataFrame` is comprised of many in-memory pandas `DataFrames` separated along the index. One operation on a Dask `DataFrame` triggers many pandas operations on the constituent pandas `DataFrame`s in a way that is mindful of potential parallelism and memory constraints.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
    "* [DataFrame screencast](https://youtu.be/AT2XtFehFSQ)\n",
    "* [DataFrame API](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "* [DataFrame examples](https://examples.dask.org/dataframe.html)\n",
    "* [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "\n",
    "**Main Take-aways**\n",
    "\n",
    "1.  Dask DataFrame should be familiar to Pandas users\n",
    "2.  The partitioning of dataframes is important for efficient execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-image in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: graphviz in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: ipycytoscape in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (1.3.3)\n",
      "Requirement already satisfied: dask[complete] in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (2023.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from h5py) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.8 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from scikit-image) (1.11.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from scikit-image) (10.0.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from scikit-image) (2.32.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from scikit-image) (2023.9.26)\n",
      "Requirement already satisfied: packaging>=21 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from scikit-image) (23.1)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from scikit-image) (0.3)\n",
      "Requirement already satisfied: ipywidgets>=7.6.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipycytoscape) (8.1.1)\n",
      "Requirement already satisfied: spectate>=1.0.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipycytoscape) (1.0.1)\n",
      "Requirement already satisfied: click>=8.1 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (8.1.6)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (2023.10.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (6.8.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (0.5)\n",
      "Requirement already satisfied: lz4>=4.3.2 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (4.3.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask[complete]) (3.17.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipycytoscape) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipycytoscape) (8.14.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipycytoscape) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipycytoscape) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipywidgets>=7.6.0->ipycytoscape) (3.0.9)\n",
      "Requirement already satisfied: locket in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from partd>=1.2.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: pandas>=1.3 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (2.0.3)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (3.3.1)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (3.1.2)\n",
      "Requirement already satisfied: distributed==2023.11.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from dask[complete]) (2023.11.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from distributed==2023.11.0->dask[complete]) (1.0.7)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from distributed==2023.11.0->dask[complete]) (5.9.5)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from distributed==2023.11.0->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from distributed==2023.11.0->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from distributed==2023.11.0->dask[complete]) (6.3.3)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from distributed==2023.11.0->dask[complete]) (2.0.4)\n",
      "Requirement already satisfied: zict>=3.0.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from distributed==2023.11.0->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from bokeh>=2.4.2->dask[complete]) (1.1.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from bokeh>=2.4.2->dask[complete]) (2023.10.1)\n",
      "Requirement already satisfied: backcall in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (0.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from jinja2>=2.10.3->dask[complete]) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from pandas>=1.3->dask[complete]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from pandas>=1.3->dask[complete]) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from pandas>=1.3->dask[complete]) (2023.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.3->dask[complete]) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.6.0->ipycytoscape) (0.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install h5py scikit-image graphviz ipycytoscape 'dask[complete]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prep.py -d flights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luyan/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 63872 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create artifical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/accounts.*.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prep import accounts_csvs\n",
    "accounts_csvs()\n",
    "\n",
    "import os\n",
    "import dask\n",
    "filename = os.path.join('data', 'accounts.*.csv')\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filename includes a glob pattern `*`, so all files in the path matching that pattern will be read into the same Dask DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>names</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>Alice</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>470</td>\n",
       "      <td>Laura</td>\n",
       "      <td>3953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>George</td>\n",
       "      <td>3642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>360</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>359</td>\n",
       "      <td>Patricia</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     names  amount\n",
       "0   22     Alice     245\n",
       "1  470     Laura    3953\n",
       "2  129    George    3642\n",
       "3  360   Charlie     568\n",
       "4  359  Patricia     374"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and count number of rows\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened here?\n",
    "- Dask investigated the input path and found that there are three matching files \n",
    "- a set of jobs was intelligently created for each chunk - one per original CSV file in this case\n",
    "- each file was loaded into a pandas dataframe, had `len()` applied to it\n",
    "- the subtotals were combined to give you the final grand total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data\n",
    "\n",
    "Lets try this with an extract of flights in the USA across several years. This data is specific to flights out of the three airports in the New York City area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the respresentation of the dataframe object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>CRSElapsedTime</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: to_pyarrow_string, 2 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                          Date DayOfWeek  DepTime CRSDepTime  ArrTime CRSArrTime UniqueCarrier FlightNum  TailNum ActualElapsedTime CRSElapsedTime  AirTime ArrDelay DepDelay  Origin    Dest Distance   TaxiIn  TaxiOut Cancelled Diverted\n",
       "npartitions=10                                                                                                                                                                                                                             \n",
       "                datetime64[ns]     int64  float64      int64  float64      int64        string     int64  float64           float64          int64  float64  float64  float64  string  string  float64  float64  float64     int64    int64\n",
       "                           ...       ...      ...        ...      ...        ...           ...       ...      ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "...                        ...       ...      ...        ...      ...        ...           ...       ...      ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "                           ...       ...      ...        ...      ...        ...           ...       ...      ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "                           ...       ...      ...        ...      ...        ...           ...       ...      ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "Dask Name: to_pyarrow_string, 2 graph layers"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the start and end of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>...</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1621.0</td>\n",
       "      <td>1540</td>\n",
       "      <td>1747.0</td>\n",
       "      <td>1701</td>\n",
       "      <td>US</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>1540</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1701</td>\n",
       "      <td>US</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-01-03</td>\n",
       "      <td>3</td>\n",
       "      <td>1546.0</td>\n",
       "      <td>1540</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>1701</td>\n",
       "      <td>US</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-01-04</td>\n",
       "      <td>4</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>1540</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>1701</td>\n",
       "      <td>US</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>1549.0</td>\n",
       "      <td>1540</td>\n",
       "      <td>1706.0</td>\n",
       "      <td>1701</td>\n",
       "      <td>US</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  DayOfWeek  DepTime  CRSDepTime  ArrTime  CRSArrTime  \\\n",
       "0 1990-01-01          1   1621.0        1540   1747.0        1701   \n",
       "1 1990-01-02          2   1547.0        1540   1700.0        1701   \n",
       "2 1990-01-03          3   1546.0        1540   1710.0        1701   \n",
       "3 1990-01-04          4   1542.0        1540   1710.0        1701   \n",
       "4 1990-01-05          5   1549.0        1540   1706.0        1701   \n",
       "\n",
       "  UniqueCarrier  FlightNum  TailNum  ActualElapsedTime  ...  AirTime  \\\n",
       "0            US         33      NaN               86.0  ...      NaN   \n",
       "1            US         33      NaN               73.0  ...      NaN   \n",
       "2            US         33      NaN               84.0  ...      NaN   \n",
       "3            US         33      NaN               88.0  ...      NaN   \n",
       "4            US         33      NaN               77.0  ...      NaN   \n",
       "\n",
       "   ArrDelay  DepDelay  Origin Dest Distance  TaxiIn  TaxiOut  Cancelled  \\\n",
       "0      46.0      41.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "1      -1.0       7.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "2       9.0       6.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "3       9.0       2.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "4       5.0       9.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "\n",
       "   Diverted  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 14:54:36,813 - distributed.worker - WARNING - Compute Failed\n",
      "Key:       ('to_pyarrow_string-7681f2f9a2baefbbf167251ca16e3d52', 9)\n",
      "Function:  execute_task\n",
      "args:      ((subgraph_callable-668d46e8-0cbc-41a9-8d1a-219aa035acba, [(<function read_block_from_file at 0x114b7aa20>, <OpenFile '/Users/luyan/Documents/UM/23Fall/si618/inclass/day_12_distributed_computing/SI_618_Day_12_dask/data/nycflights/1999.csv'>, 0, 25952466, b'\\n'), None, True, True]))\n",
      "kwargs:    {}\n",
      "Exception: 'ValueError(\\'Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\\\\n\\\\n+----------------+---------+----------+\\\\n| Column         | Found   | Expected |\\\\n+----------------+---------+----------+\\\\n| CRSElapsedTime | float64 | int64    |\\\\n| TailNum        | object  | float64  |\\\\n+----------------+---------+----------+\\\\n\\\\nThe following columns also raised exceptions on conversion:\\\\n\\\\n- TailNum\\\\n  ValueError(\"could not convert string to float: \\\\\\'N54711\\\\\\'\")\\\\n\\\\nUsually this is due to dask\\\\\\'s dtype inference failing, and\\\\n*may* be fixed by specifying dtypes manually by adding:\\\\n\\\\ndtype={\\\\\\'CRSElapsedTime\\\\\\': \\\\\\'float64\\\\\\',\\\\n       \\\\\\'TailNum\\\\\\': \\\\\\'object\\\\\\'}\\\\n\\\\nto the call to `read_csv`/`read_table`.\\')'\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+----------------+---------+----------+\n| Column         | Found   | Expected |\n+----------------+---------+----------+\n| CRSElapsedTime | float64 | int64    |\n| TailNum        | object  | float64  |\n+----------------+---------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- TailNum\n  ValueError(\"could not convert string to float: 'N54711'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'CRSElapsedTime': 'float64',\n       'TailNum': 'object'}\n\nto the call to `read_csv`/`read_table`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/luyan/Documents/UM/23Fall/si618/inclass/day_12_distributed_computing/SI_618_Day_12_dask/SI_618_Day_12_Distributed_Computing.ipynb 单元格 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luyan/Documents/UM/23Fall/si618/inclass/day_12_distributed_computing/SI_618_Day_12_dask/SI_618_Day_12_Distributed_Computing.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mtail()  \u001b[39m# this fails\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/dask/dataframe/core.py:1589\u001b[0m, in \u001b[0;36m_Frame.tail\u001b[0;34m(self, n, compute)\u001b[0m\n\u001b[1;32m   1586\u001b[0m result \u001b[39m=\u001b[39m new_dd_object(graph, name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_meta, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdivisions[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:])\n\u001b[1;32m   1588\u001b[0m \u001b[39mif\u001b[39;00m compute:\n\u001b[0;32m-> 1589\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49mcompute()\n\u001b[1;32m   1590\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/dask/base.py:342\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    319\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[1;32m    321\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/dask/base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    627\u001b[0m \u001b[39mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 628\u001b[0m     results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    630\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/dask/dataframe/io/csv.py:142\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m         rest_kwargs[\u001b[39m\"\u001b[39m\u001b[39musecols\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m columns\n\u001b[1;32m    141\u001b[0m \u001b[39m# Call `pandas_read_text`\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m df \u001b[39m=\u001b[39m pandas_read_text(\n\u001b[1;32m    143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader,\n\u001b[1;32m    144\u001b[0m     block,\n\u001b[1;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheader,\n\u001b[1;32m    146\u001b[0m     rest_kwargs,\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtypes,\n\u001b[1;32m    148\u001b[0m     columns,\n\u001b[1;32m    149\u001b[0m     write_header,\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menforce,\n\u001b[1;32m    151\u001b[0m     path_info,\n\u001b[1;32m    152\u001b[0m )\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m project_after_read:\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m df[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/dask/dataframe/io/csv.py:197\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m df \u001b[39m=\u001b[39m reader(bio, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    196\u001b[0m \u001b[39mif\u001b[39;00m dtypes:\n\u001b[0;32m--> 197\u001b[0m     coerce_dtypes(df, dtypes)\n\u001b[1;32m    199\u001b[0m \u001b[39mif\u001b[39;00m enforce \u001b[39mand\u001b[39;00m columns \u001b[39mand\u001b[39;00m (\u001b[39mlist\u001b[39m(df\u001b[39m.\u001b[39mcolumns) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m(columns)):\n\u001b[1;32m    200\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mColumns do not match\u001b[39m\u001b[39m\"\u001b[39m, df\u001b[39m.\u001b[39mcolumns, columns)\n",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/dask/dataframe/io/csv.py:298\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m rule \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m61\u001b[39m)\n\u001b[1;32m    295\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[1;32m    296\u001b[0m     rule\u001b[39m.\u001b[39mjoin(\u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[1;32m    297\u001b[0m )\n\u001b[0;32m--> 298\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+----------------+---------+----------+\n| Column         | Found   | Expected |\n+----------------+---------+----------+\n| CRSElapsedTime | float64 | int64    |\n| TailNum        | object  | float64  |\n+----------------+---------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- TailNum\n  ValueError(\"could not convert string to float: 'N54711'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'CRSElapsedTime': 'float64',\n       'TailNum': 'object'}\n\nto the call to `read_csv`/`read_table`."
     ]
    }
   ],
   "source": [
    "df.tail()  # this fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "Unlike `pandas.read_csv` which reads in the entire file before inferring datatypes, `dask.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file if using a glob). These inferred datatypes are then enforced when reading all partitions.\n",
    "\n",
    "In this case, the datatypes inferred in the sample are incorrect. The first `n` rows have no value for `CRSElapsedTime` (which pandas infers as a `float`), and later on turn out to be strings (`object` dtype). Note that Dask gives an informative error message about the mismatch. When this happens you have a few options:\n",
    "\n",
    "- Specify dtypes directly using the `dtype` keyword. This is the recommended solution, as it's the least error prone (better to be explicit than implicit) and also the most performant.\n",
    "- Increase the size of the `sample` keyword (in bytes)\n",
    "- Use `assume_missing` to make `dask` assume that columns inferred to be `int` (which don't allow missing values) are actually floats (which do allow missing values). In our particular case this doesn't apply.\n",
    "\n",
    "In our case we'll use the first option and directly specify the `dtypes` of the offending columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>...</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269176</th>\n",
       "      <td>1999-12-27</td>\n",
       "      <td>1</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>1645</td>\n",
       "      <td>1830.0</td>\n",
       "      <td>1901</td>\n",
       "      <td>UA</td>\n",
       "      <td>1753</td>\n",
       "      <td>N516UA</td>\n",
       "      <td>225.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269177</th>\n",
       "      <td>1999-12-28</td>\n",
       "      <td>2</td>\n",
       "      <td>1726.0</td>\n",
       "      <td>1645</td>\n",
       "      <td>1928.0</td>\n",
       "      <td>1901</td>\n",
       "      <td>UA</td>\n",
       "      <td>1753</td>\n",
       "      <td>N504UA</td>\n",
       "      <td>242.0</td>\n",
       "      <td>...</td>\n",
       "      <td>214.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269178</th>\n",
       "      <td>1999-12-29</td>\n",
       "      <td>3</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>1645</td>\n",
       "      <td>1846.0</td>\n",
       "      <td>1901</td>\n",
       "      <td>UA</td>\n",
       "      <td>1753</td>\n",
       "      <td>N592UA</td>\n",
       "      <td>240.0</td>\n",
       "      <td>...</td>\n",
       "      <td>220.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269179</th>\n",
       "      <td>1999-12-30</td>\n",
       "      <td>4</td>\n",
       "      <td>1651.0</td>\n",
       "      <td>1645</td>\n",
       "      <td>1908.0</td>\n",
       "      <td>1901</td>\n",
       "      <td>UA</td>\n",
       "      <td>1753</td>\n",
       "      <td>N575UA</td>\n",
       "      <td>257.0</td>\n",
       "      <td>...</td>\n",
       "      <td>233.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269180</th>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>5</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>1645</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>1901</td>\n",
       "      <td>UA</td>\n",
       "      <td>1753</td>\n",
       "      <td>N539UA</td>\n",
       "      <td>249.0</td>\n",
       "      <td>...</td>\n",
       "      <td>232.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  DayOfWeek  DepTime  CRSDepTime  ArrTime  CRSArrTime  \\\n",
       "269176 1999-12-27          1   1645.0        1645   1830.0        1901   \n",
       "269177 1999-12-28          2   1726.0        1645   1928.0        1901   \n",
       "269178 1999-12-29          3   1646.0        1645   1846.0        1901   \n",
       "269179 1999-12-30          4   1651.0        1645   1908.0        1901   \n",
       "269180 1999-12-31          5   1642.0        1645   1851.0        1901   \n",
       "\n",
       "       UniqueCarrier  FlightNum TailNum  ActualElapsedTime  ...  AirTime  \\\n",
       "269176            UA       1753  N516UA              225.0  ...    205.0   \n",
       "269177            UA       1753  N504UA              242.0  ...    214.0   \n",
       "269178            UA       1753  N592UA              240.0  ...    220.0   \n",
       "269179            UA       1753  N575UA              257.0  ...    233.0   \n",
       "269180            UA       1753  N539UA              249.0  ...    232.0   \n",
       "\n",
       "        ArrDelay  DepDelay  Origin Dest Distance  TaxiIn  TaxiOut  Cancelled  \\\n",
       "269176     -31.0       0.0     LGA  DEN   1619.0     7.0     13.0      False   \n",
       "269177      27.0      41.0     LGA  DEN   1619.0     5.0     23.0      False   \n",
       "269178     -15.0       1.0     LGA  DEN   1619.0     5.0     15.0      False   \n",
       "269179       7.0       6.0     LGA  DEN   1619.0     5.0     19.0      False   \n",
       "269180     -10.0      -3.0     LGA  DEN   1619.0     6.0     11.0      False   \n",
       "\n",
       "        Diverted  \n",
       "269176         0  \n",
       "269177         0  \n",
       "269178         0  \n",
       "269179         0  \n",
       "269180         0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()  # now works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computations with `dask.dataframe`\n",
    "\n",
    "We compute the maximum of the `DepDelay` column. With just pandas, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums\n",
    "\n",
    "```python\n",
    "maxes = []\n",
    "for fn in filenames:\n",
    "    df = pd.read_csv(fn)\n",
    "    maxes.append(df.DepDelay.max())\n",
    "    \n",
    "final_max = max(maxes)\n",
    "```\n",
    "\n",
    "We could wrap that `pd.read_csv` with `dask.delayed` so that it runs in parallel. Regardless, we're still having to think about loops, intermediate results (one per file) and the final reduction (`max` of the intermediate maxes). This is just noise around the real task, which pandas solves with\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(filename, dtype=dtype)\n",
    "df.DepDelay.max()\n",
    "```\n",
    "\n",
    "`dask.dataframe` lets us write pandas-like code, that operates on larger than memory datasets in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime',\n",
       "       'UniqueCarrier', 'FlightNum', 'TailNum', 'ActualElapsedTime',\n",
       "       'CRSElapsedTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest',\n",
       "       'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'Diverted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<series-..., dtype=float64>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.DepDelay.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1435.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.DepDelay.max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2611892"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This writes the delayed computation for us and then runs it.  \n",
    "\n",
    "Some things to note:\n",
    "\n",
    "1.  As with `dask.delayed`, we need to call `.compute()` when we're done.  Up until this point everything is lazy.\n",
    "2.  Dask will delete intermediate results (like the full pandas dataframe for each file) as soon as possible.\n",
    "    -  This lets us handle datasets that are larger than memory\n",
    "    -  This means that repeated computations will have to load all of the data in each time (run the code above again, is it faster or slower than you would expect?)\n",
    "    \n",
    "As with `Delayed` objects, you can view the underlying task graph using the `.visualize` method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In this section we do a few `dask.dataframe` computations. If you are comfortable with Pandas then these should be familiar. You will have to think about when to call `compute`.\n",
    "\n",
    "### 1.) How many rows are in our dataset?\n",
    "\n",
    "If you aren't familiar with pandas, how would you check how many records are in a list of tuples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2611892"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert your code here\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) In total, how many non-canceled flights were taken?\n",
    "\n",
    "With pandas, you would use [boolean indexing](https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70931"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert your code here\n",
    "len(df[df['Cancelled'] == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) In total, how many non-cancelled flights were taken from each airport?\n",
    "\n",
    "*Hint*: use [`df.groupby`](https://pandas.pydata.org/pandas-docs/stable/groupby.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Origin\n",
       "EWR    1174131\n",
       "LGA    1003420\n",
       "JFK     434341\n",
       "Name: count, dtype: int64[pyarrow]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert your code here\n",
    "df['Origin'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Origin\n",
       "EWR    1139451\n",
       "LGA     974267\n",
       "JFK     427243\n",
       "Name: count, dtype: int64[pyarrow]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[~df['Cancelled']].Origin.value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) What was the average departure delay from each airport?\n",
    "\n",
    "Note, this is the same computation you did in the previous notebook (is this approach faster or slower?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime',\n",
       "       'UniqueCarrier', 'FlightNum', 'TailNum', 'ActualElapsedTime',\n",
       "       'CRSElapsedTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest',\n",
       "       'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'Diverted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Origin\n",
       "EWR    10.295469\n",
       "JFK    10.351299\n",
       "LGA     7.431142\n",
       "Name: DepDelay, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert your code here\n",
    "df.groupby('Origin').DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) What day of the week has the worst average departure delay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DayOfWeek\n",
       "1     8.096565\n",
       "2     8.149109\n",
       "3     9.141912\n",
       "4    10.538275\n",
       "5    11.476687\n",
       "6     7.824071\n",
       "7     8.994296\n",
       "Name: DepDelay, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert your code here\n",
    "df.groupby('DayOfWeek').DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Intermediate Results\n",
    "\n",
    "When computing all of the above, we sometimes did the same operation more than once. For most operations, `dask.dataframe` hashes the arguments, allowing duplicate computations to be shared, and only computed once.\n",
    "\n",
    "For example, lets compute the mean and standard deviation for departure delay of all non-canceled flights. Since dask operations are lazy, those values aren't the final results yet. They're just the recipe required to get the result.\n",
    "\n",
    "If we compute them with two calls to compute, there is no sharing of intermediate computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = df[~df.Cancelled]\n",
    "mean_delay = non_cancelled.DepDelay.mean()\n",
    "std_delay = non_cancelled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<series-..., dtype=float64>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.16 s, sys: 412 ms, total: 1.57 s\n",
      "Wall time: 7.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's try by passing both to a single `compute` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 497 ms, sys: 157 ms, total: 654 ms\n",
      "Wall time: 2.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `dask.compute` takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling `dask.compute`, allowing shared operations to only be done once instead of twice. In particular, using `dask.compute` only does the following once:\n",
    "\n",
    "- the calls to `read_csv`\n",
    "- the filter (`df[~df.Cancelled]`)\n",
    "- some of the necessary reductions (`sum`, `count`)\n",
    "\n",
    "To see what the merged task graphs between multiple results look like (and what's shared), you can use the `dask.visualize` function (we might want to use `filename='graph.pdf'` to save the graph to disk so that we can zoom in more easily):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this compare to Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is more mature and fully featured than `dask.dataframe`.  If your data fits in memory then you should use Pandas.  The `dask.dataframe` module gives you a limited `pandas` experience when you operate on datasets that don't fit comfortably in memory.\n",
    "\n",
    "During this tutorial we provide a small dataset consisting of a few CSV files.  This dataset is 45MB on disk that expands to about 400MB in memory. This dataset is small enough that you would normally use Pandas.\n",
    "\n",
    "We've chosen this size so that exercises finish quickly.  Dask.dataframe only really becomes meaningful for problems significantly larger than this, when Pandas breaks with the dreaded \n",
    "\n",
    "    MemoryError:  ...\n",
    "    \n",
    "Furthermore, the distributed scheduler allows the same dataframe expressions to be executed across a cluster. To enable massive \"big data\" processing, one could execute data ingestion functions such as `read_csv`, where the data is held on storage accessible to every worker node (e.g., amazon's S3), and because most operations begin by selecting only some columns, transforming and filtering the data, only relatively small amounts of data need to be communicated between the machines.\n",
    "\n",
    "Dask.dataframe operations use `pandas` operations internally.  Generally they run at about the same speed except in the following two cases:\n",
    "\n",
    "1.  Dask introduces a bit of overhead, around 1ms per task.  This is usually negligible.\n",
    "2.  When Pandas releases the GIL `dask.dataframe` can call several pandas operations in parallel within a process, increasing speed somewhat proportional to the number of cores. For operations which don't release the GIL, multiple processes would be needed to get the same speedup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask DataFrame Data Model\n",
    "\n",
    "For the most part, a Dask DataFrame feels like a pandas DataFrame.\n",
    "So far, the biggest difference we've seen is that Dask operations are lazy; they build up a task graph instead of executing immediately (more details coming in [Schedulers](05_distributed.ipynb)).\n",
    "This lets Dask do operations in parallel and out of core.\n",
    "\n",
    "In [Dask Arrays](03_array.ipynb), we saw that a `dask.array` was composed of many NumPy arrays, chunked along one or more dimensions.\n",
    "It's similar for `dask.dataframe`: a Dask DataFrame is composed of many pandas DataFrames. For `dask.dataframe` the chunking happens only along the index.\n",
    "\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" width=\"30%\">\n",
    "\n",
    "We call each chunk a *partition*, and the upper / lower bounds are *divisions*.\n",
    "Dask *can* store information about the divisions. For now, partitions come up when you write custom functions to apply to Dask DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting `CRSDepTime` to a timestamp\n",
    "\n",
    "This dataset stores timestamps as `HHMM`, which are read in as integers in `read_csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1540\n",
       "1    1540\n",
       "2    1540\n",
       "3    1540\n",
       "4    1540\n",
       "5    1540\n",
       "6    1540\n",
       "7    1540\n",
       "8    1540\n",
       "9    1540\n",
       "Name: CRSDepTime, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crs_dep_time = df.CRSDepTime.head(10)\n",
    "crs_dep_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert these to timestamps of scheduled departure time, we need to convert these integers into `pd.Timedelta` objects, and then combine them with the `Date` column.\n",
    "\n",
    "In pandas we'd do this using the `pd.to_timedelta` function, and a bit of arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   1990-01-01 15:40:00\n",
       "1   1990-01-02 15:40:00\n",
       "2   1990-01-03 15:40:00\n",
       "3   1990-01-04 15:40:00\n",
       "4   1990-01-05 15:40:00\n",
       "5   1990-01-06 15:40:00\n",
       "6   1990-01-07 15:40:00\n",
       "7   1990-01-08 15:40:00\n",
       "8   1990-01-09 15:40:00\n",
       "9   1990-01-10 15:40:00\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the first 10 dates to complement our `crs_dep_time`\n",
    "date = df.Date.head(10)\n",
    "\n",
    "# Get hours as an integer, convert to a timedelta\n",
    "hours = crs_dep_time // 100\n",
    "hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
    "\n",
    "# Get minutes as an integer, convert to a timedelta\n",
    "minutes = crs_dep_time % 100\n",
    "minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
    "\n",
    "# Apply the timedeltas to offset the dates by the departure time\n",
    "departure_timestamp = date + hours_timedelta + minutes_timedelta\n",
    "departure_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom code and Dask Dataframe\n",
    "\n",
    "We could swap out `pd.to_timedelta` for `dd.to_timedelta` and do the same operations on the entire dask DataFrame. But let's say that Dask hadn't implemented a `dd.to_timedelta` that works on Dask DataFrames. What would you do then?\n",
    "\n",
    "`dask.dataframe` provides a few methods to make applying custom functions to Dask DataFrames easier:\n",
    "\n",
    "- [`map_partitions`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions)\n",
    "- [`map_overlap`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_overlap)\n",
    "- [`reduction`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.reduction)\n",
    "\n",
    "Here we'll just be discussing `map_partitions`, which we can use to implement `to_timedelta` on our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method map_partitions in module dask.dataframe.core:\n",
      "\n",
      "map_partitions(func, *args, **kwargs) method of dask.dataframe.core.Series instance\n",
      "    Apply Python function on each DataFrame partition.\n",
      "    \n",
      "    Note that the index and divisions are assumed to remain unchanged.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    func : function\n",
      "        The function applied to each partition. If this function accepts\n",
      "        the special ``partition_info`` keyword argument, it will receive\n",
      "        information on the partition's relative location within the\n",
      "        dataframe.\n",
      "    args, kwargs :\n",
      "        Positional and keyword arguments to pass to the function.\n",
      "        Positional arguments are computed on a per-partition basis, while\n",
      "        keyword arguments are shared across all partitions. The partition\n",
      "        itself will be the first positional argument, with all other\n",
      "        arguments passed *after*. Arguments can be ``Scalar``, ``Delayed``,\n",
      "        or regular Python objects. DataFrame-like args (both dask and\n",
      "        pandas) will be repartitioned to align (if necessary) before\n",
      "        applying the function; see ``align_dataframes`` to control this\n",
      "        behavior.\n",
      "    enforce_metadata : bool, default True\n",
      "        Whether to enforce at runtime that the structure of the DataFrame\n",
      "        produced by ``func`` actually matches the structure of ``meta``.\n",
      "        This will rename and reorder columns for each partition,\n",
      "        and will raise an error if this doesn't work,\n",
      "        but it won't raise if dtypes don't match.\n",
      "    transform_divisions : bool, default True\n",
      "        Whether to apply the function onto the divisions and apply those\n",
      "        transformed divisions to the output.\n",
      "    align_dataframes : bool, default True\n",
      "        Whether to repartition DataFrame- or Series-like args\n",
      "        (both dask and pandas) so their divisions align before applying\n",
      "        the function. This requires all inputs to have known divisions.\n",
      "        Single-partition inputs will be split into multiple partitions.\n",
      "    \n",
      "        If False, all inputs must have either the same number of partitions\n",
      "        or a single partition. Single-partition inputs will be broadcast to\n",
      "        every partition of multi-partition inputs.\n",
      "    meta : pd.DataFrame, pd.Series, dict, iterable, tuple, optional\n",
      "        An empty ``pd.DataFrame`` or ``pd.Series`` that matches the dtypes\n",
      "        and column names of the output. This metadata is necessary for\n",
      "        many algorithms in dask dataframe to work.  For ease of use, some\n",
      "        alternative inputs are also available. Instead of a ``DataFrame``,\n",
      "        a ``dict`` of ``{name: dtype}`` or iterable of ``(name, dtype)``\n",
      "        can be provided (note that the order of the names should match the\n",
      "        order of the columns). Instead of a series, a tuple of ``(name,\n",
      "        dtype)`` can be used. If not provided, dask will try to infer the\n",
      "        metadata. This may lead to unexpected results, so providing\n",
      "        ``meta`` is recommended. For more information, see\n",
      "        ``dask.dataframe.utils.make_meta``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Given a DataFrame, Series, or Index, such as:\n",
      "    \n",
      "    >>> import pandas as pd\n",
      "    >>> import dask.dataframe as dd\n",
      "    >>> df = pd.DataFrame({'x': [1, 2, 3, 4, 5],\n",
      "    ...                    'y': [1., 2., 3., 4., 5.]})\n",
      "    >>> ddf = dd.from_pandas(df, npartitions=2)\n",
      "    \n",
      "    One can use ``map_partitions`` to apply a function on each partition.\n",
      "    Extra arguments and keywords can optionally be provided, and will be\n",
      "    passed to the function after the partition.\n",
      "    \n",
      "    Here we apply a function with arguments and keywords to a DataFrame,\n",
      "    resulting in a Series:\n",
      "    \n",
      "    >>> def myadd(df, a, b=1):\n",
      "    ...     return df.x + df.y + a + b\n",
      "    >>> res = ddf.map_partitions(myadd, 1, b=2)\n",
      "    >>> res.dtype\n",
      "    dtype('float64')\n",
      "    \n",
      "    Here we apply a function to a Series resulting in a Series:\n",
      "    \n",
      "    >>> res = ddf.x.map_partitions(lambda x: len(x)) # ddf.x is a Dask Series Structure\n",
      "    >>> res.dtype\n",
      "    dtype('int64')\n",
      "    \n",
      "    By default, dask tries to infer the output metadata by running your\n",
      "    provided function on some fake data. This works well in many cases, but\n",
      "    can sometimes be expensive, or even fail. To avoid this, you can\n",
      "    manually specify the output metadata with the ``meta`` keyword. This\n",
      "    can be specified in many forms, for more information see\n",
      "    ``dask.dataframe.utils.make_meta``.\n",
      "    \n",
      "    Here we specify the output is a Series with no name, and dtype\n",
      "    ``float64``:\n",
      "    \n",
      "    >>> res = ddf.map_partitions(myadd, 1, b=2, meta=(None, 'f8'))\n",
      "    \n",
      "    Here we map a function that takes in a DataFrame, and returns a\n",
      "    DataFrame with a new column:\n",
      "    \n",
      "    >>> res = ddf.map_partitions(lambda df: df.assign(z=df.x * df.y))\n",
      "    >>> res.dtypes\n",
      "    x      int64\n",
      "    y    float64\n",
      "    z    float64\n",
      "    dtype: object\n",
      "    \n",
      "    As before, the output metadata can also be specified manually. This\n",
      "    time we pass in a ``dict``, as the output is a DataFrame:\n",
      "    \n",
      "    >>> res = ddf.map_partitions(lambda df: df.assign(z=df.x * df.y),\n",
      "    ...                          meta={'x': 'i8', 'y': 'f8', 'z': 'f8'})\n",
      "    \n",
      "    In the case where the metadata doesn't change, you can also pass in\n",
      "    the object itself directly:\n",
      "    \n",
      "    >>> res = ddf.map_partitions(lambda df: df.head(), meta=ddf)\n",
      "    \n",
      "    Also note that the index and divisions are assumed to remain unchanged.\n",
      "    If the function you're mapping changes the index/divisions, you'll need\n",
      "    to clear them afterwards:\n",
      "    \n",
      "    >>> ddf.map_partitions(func).clear_divisions()  # doctest: +SKIP\n",
      "    \n",
      "    Your map function gets information about where it is in the dataframe by\n",
      "    accepting a special ``partition_info`` keyword argument.\n",
      "    \n",
      "    >>> def func(partition, partition_info=None):\n",
      "    ...     pass\n",
      "    \n",
      "    This will receive the following information:\n",
      "    \n",
      "    >>> partition_info  # doctest: +SKIP\n",
      "    {'number': 1, 'division': 3}\n",
      "    \n",
      "    For each argument and keyword arguments that are dask dataframes you will\n",
      "    receive the number (n) which represents the nth partition of the dataframe\n",
      "    and the division (the first index value in the partition). If divisions\n",
      "    are not known (for instance if the index is not sorted) then you will get\n",
      "    None as the division.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the docs for `map_partitions`\n",
    "\n",
    "help(df.CRSDepTime.map_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to apply a function that operates on a DataFrame to each partition.\n",
    "In this case, we'll apply `pd.to_timedelta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = df.CRSDepTime // 100\n",
    "# hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
    "hours_timedelta = hours.map_partitions(pd.to_timedelta, unit='h')\n",
    "\n",
    "minutes = df.CRSDepTime % 100\n",
    "# minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
    "minutes_timedelta = minutes.map_partitions(pd.to_timedelta, unit='m')\n",
    "\n",
    "departure_timestamp = df.Date + hours_timedelta + minutes_timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=10\n",
       "    datetime64[ns]\n",
       "               ...\n",
       "         ...      \n",
       "               ...\n",
       "               ...\n",
       "dtype: datetime64[ns]\n",
       "Dask Name: add, 10 graph layers"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departure_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   1990-01-01 15:40:00\n",
       "1   1990-01-02 15:40:00\n",
       "2   1990-01-03 15:40:00\n",
       "3   1990-01-04 15:40:00\n",
       "4   1990-01-05 15:40:00\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departure_timestamp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Rewrite above to use a single call to `map_partitions`\n",
    "\n",
    "This will be slightly more efficient than two separate calls, as it reduces the number of tasks in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_departure_timestamp(df):\n",
    "    pass  # TODO: implement this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# departure_timestamp = df.map_partitions(compute_departure_timestamp)\n",
    "\n",
    "# departure_timestamp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What doesn't work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask.dataframe only covers a small but well-used portion of the Pandas API.\n",
    "This limitation is for two reasons:\n",
    "\n",
    "1.  The Pandas API is *huge*\n",
    "2.  Some operations are genuinely hard to do in parallel (e.g. sort)\n",
    "\n",
    "Additionally, some important operations like ``set_index`` work, but are slower\n",
    "than in Pandas because they include substantial shuffling of data, and may write out to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learn More\n",
    "\n",
    "\n",
    "* [DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
    "* [DataFrame screencast](https://youtu.be/AT2XtFehFSQ)\n",
    "* [DataFrame API](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "* [DataFrame examples](https://examples.dask.org/dataframe.html)\n",
    "* [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing scikit-learn from dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a (fake) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.precision = 2\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = dask.datasets.timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=30</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01</th>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-30</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-31</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: to_pyarrow_string, 2 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                  name     id        x        y\n",
       "npartitions=30                                 \n",
       "2000-01-01      string  int64  float64  float64\n",
       "2000-01-02         ...    ...      ...      ...\n",
       "...                ...    ...      ...      ...\n",
       "2000-01-30         ...    ...      ...      ...\n",
       "2000-01-31         ...    ...      ...      ...\n",
       "Dask Name: to_pyarrow_string, 2 graph layers"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:00</th>\n",
       "      <td>Tim</td>\n",
       "      <td>940</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:01</th>\n",
       "      <td>Edith</td>\n",
       "      <td>1024</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:02</th>\n",
       "      <td>Wendy</td>\n",
       "      <td>983</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:03</th>\n",
       "      <td>Norbert</td>\n",
       "      <td>969</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:04</th>\n",
       "      <td>Frank</td>\n",
       "      <td>1011</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name    id     x     y\n",
       "timestamp                                     \n",
       "2000-01-01 00:00:00      Tim   940 -0.40  0.29\n",
       "2000-01-01 00:00:01    Edith  1024 -0.42 -0.90\n",
       "2000-01-01 00:00:02    Wendy   983  0.14 -0.03\n",
       "2000-01-01 00:00:03  Norbert   969  0.15  0.07\n",
       "2000-01-01 00:00:04    Frank  1011 -0.05  0.06"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ts.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=26</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zelda</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zelda</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: sort_index, 8 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                   id        x        y\n",
       "npartitions=26                         \n",
       "Alice           int64  float64  float64\n",
       "Bob               ...      ...      ...\n",
       "...               ...      ...      ...\n",
       "Zelda             ...      ...      ...\n",
       "Zelda             ...      ...      ...\n",
       "Dask Name: sort_index, 8 graph layers"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def train(partition):\n",
    "    if not len(partition):\n",
    "        return\n",
    "    est = LinearRegression()\n",
    "    est.fit(partition[[\"x\"]].values, partition.y.values)\n",
    "    return est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "Fit a linear regression that models `y` as a function of `x` *for each name* and create two new columns: one for the (x-)coefficient and one for the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ts.map_partitions(train).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     LinearRegression()\n",
       "1     LinearRegression()\n",
       "2     LinearRegression()\n",
       "3     LinearRegression()\n",
       "4     LinearRegression()\n",
       "             ...        \n",
       "21    LinearRegression()\n",
       "22    LinearRegression()\n",
       "23    LinearRegression()\n",
       "24    LinearRegression()\n",
       "25    LinearRegression()\n",
       "Length: 26, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [-0.0010068405254295078]\n",
       "1       [-0.003532664401981819]\n",
       "2       [0.0024642055955763948]\n",
       "3     [-0.00031300224828905083]\n",
       "4         [0.00297219103686713]\n",
       "                ...            \n",
       "21       [0.004276112265963434]\n",
       "22     [-0.0024424058793617504]\n",
       "23      [0.0027103196502247656]\n",
       "24     [-0.0019166578413009638]\n",
       "25    [-0.00021128634907650293]\n",
       "Length: 26, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: x.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.84e-03\n",
       "1     2.21e-03\n",
       "2     2.77e-04\n",
       "3    -3.55e-03\n",
       "4     1.08e-03\n",
       "        ...   \n",
       "21    1.85e-03\n",
       "22    1.80e-03\n",
       "23    2.47e-03\n",
       "24   -3.09e-03\n",
       "25   -1.89e-04\n",
       "Length: 26, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: x.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/luyan/Documents/UM/23Fall/si618/inclass/day_12_distributed_computing/SI_618_Day_12_dask/SI_618_Day_12_Distributed_Computing.ipynb 单元格 84\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luyan/Documents/UM/23Fall/si618/inclass/day_12_distributed_computing/SI_618_Day_12_dask/SI_618_Day_12_Distributed_Computing.ipynb#Y145sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ts[\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mcompute()\n",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/dask/dataframe/core.py:5041\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5038\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc[key]\n\u001b[1;32m   5040\u001b[0m \u001b[39m# error is raised from pandas\u001b[39;00m\n\u001b[0;32m-> 5041\u001b[0m meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_meta[_extract_meta(key)]\n\u001b[1;32m   5042\u001b[0m dsk \u001b[39m=\u001b[39m partitionwise_graph(operator\u001b[39m.\u001b[39mgetitem, name, \u001b[39mself\u001b[39m, key)\n\u001b[1;32m   5043\u001b[0m graph \u001b[39m=\u001b[39m HighLevelGraph\u001b[39m.\u001b[39mfrom_collections(name, dsk, dependencies\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/UM/23Fall/si618/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model'"
     ]
    }
   ],
   "source": [
    "ts['model'].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the next cell when you're all done (and not before!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
